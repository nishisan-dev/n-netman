# n-netman â€” Nishi Network Manager

[![Go Version](https://img.shields.io/badge/go-1.23+-blue.svg)](https://golang.org/)
[![License](https://img.shields.io/badge/license-Nishi--NC-orange.svg)](LICENSE)

**n-netman** Ã© um agente leve para criaÃ§Ã£o e gerenciamento de **overlays VXLAN L3/L2** entre hosts Linux rodando KVM/libvirt.

## ğŸ¯ Objetivo

Permitir que redes virtuais distribuÃ­das sejam criadas de forma **declarativa e simples**, eliminando a necessidade de soluÃ§Ãµes complexas como OVS.

### O que jÃ¡ funciona

- âœ… CriaÃ§Ã£o/atualizaÃ§Ã£o de interfaces VXLAN e bridges Linux
- âœ… SincronizaÃ§Ã£o de FDB para peers configurados (flooding BUM)
- âœ… BUM em head-end-replication (FDB) e multicast (grupo IP)
- âœ… **Troca de rotas via gRPC** (ExchangeState, AnnounceRoutes, WithdrawRoutes)
- âœ… InstalaÃ§Ã£o automÃ¡tica de rotas recebidas no kernel
- âœ… CLI `nnet` com `apply`, `status`, `routes`, `doctor`, `version`
- âœ… Carregamento/validaÃ§Ã£o de config YAML com defaults
- âœ… Healthchecks HTTP e endpoint de mÃ©tricas
- âœ… **Status real dos peers** via endpoint `/status` (healthy/unhealthy/disconnected)
- âœ… **EstatÃ­sticas de rotas** (exported, installed, per-peer)
- âœ… **Cleanup automÃ¡tico** de rotas no shutdown e quando peers caem (`flush_on_peer_down`)
- âœ… **Multi-Overlay (v2)** â€” MÃºltiplos VXLANs com routing independente por overlay
- âœ… Bridge com IPv4/IPv6 por overlay (para nexthop e anÃºncios)
- âœ… **TLS/mTLS** para comunicaÃ§Ã£o gRPC entre peers
- âœ… **Policy-Based Routing** â€” Rotas instaladas em tabelas especÃ­ficas por VNI
- âœ… **IntegraÃ§Ã£o libvirt** â€” Attach/detach de VMs via `nnet libvirt`

### Em progresso

- âš ï¸ Netplan parsing e rotas conectadas/estÃ¡ticas
- âš ï¸ PolÃ­ticas de import/export (`allow/deny`, `include_connected`, `include_netplan_static`)

### Ainda nÃ£o funciona (resumo rÃ¡pido)

- âŒ PolÃ­ticas avanÃ§adas de import/export (`allow/deny/accept_all`, `export_all`)
- âŒ ValidaÃ§Ã£o de PSK entre peers

---

## ğŸ“‹ PrÃ©-requisitos

### Sistema Operacional
- Linux com kernel 3.7+ (suporte a VXLAN)
- Testado em Ubuntu 22.04+, Debian 12+

### DependÃªncias
```bash
# Verificar suporte a VXLAN e bridges
lsmod | grep vxlan
lsmod | grep bridge

# Se nÃ£o estiverem carregados:
sudo modprobe vxlan
sudo modprobe bridge
```

### Build
- Go 1.23 ou superior

```bash
# Verificar versÃ£o do Go
go version
```

---

## ğŸš€ InstalaÃ§Ã£o

### OpÃ§Ã£o 1: Build do Fonte

```bash
# Clonar repositÃ³rio
git clone https://github.com/nishisan-dev/n-netman.git
cd n-netman

# Build
make build

# Ou manualmente:
go build -o bin/nnetd ./cmd/nnetd
go build -o bin/nnet ./cmd/nnet
```

### OpÃ§Ã£o 2: InstalaÃ§Ã£o no Sistema

```bash
# Build e instalar em $GOPATH/bin
make install

# Ou copiar manualmente
sudo cp bin/nnetd /usr/local/bin/
sudo cp bin/nnet /usr/local/bin/
```

---

## âš™ï¸ ConfiguraÃ§Ã£o

### Criar DiretÃ³rio de ConfiguraÃ§Ã£o

```bash
sudo mkdir -p /etc/n-netman
```

### Arquivo de ConfiguraÃ§Ã£o

Crie o arquivo `/etc/n-netman/n-netman.yaml`:

```yaml
version: 1

node:
  id: "host-a-01"          # Identificador Ãºnico deste nÃ³
  hostname: "host-a"
  tags:
    - "datacenter-1"
    - "kvm"

# IntegraÃ§Ã£o com netplan (somente leitura)
netplan:
  enabled: true
  underlay:
    prefer_interfaces:
      - "eth0"
      - "ens3"
    prefer_address_families:
      - "ipv4"

# IntegraÃ§Ã£o com KVM/libvirt (opcional)
kvm:
  enabled: false           # Defina como true se usar libvirt
  bridges:
    - name: "br-nnet-100"
      stp: false
      mtu: 1450
      manage: true

# ConfiguraÃ§Ã£o do overlay VXLAN
overlay:
  vxlan:
    vni: 100               # VXLAN Network Identifier
    name: "vxlan100"
    dstport: 4789
    mtu: 1450
    learning: true
    bridge: "br-nnet-100"

  # Peers (outros hosts no overlay)
  peers:
    - id: "host-b-01"
      endpoint:
        address: "10.10.0.12"    # IP underlay do peer
      auth:
        mode: "psk"
        psk_ref: "file:/etc/n-netman/psk/host-b-01.key"
      health:
        keepalive_interval_ms: 1500
        dead_after_ms: 6000

    - id: "host-c-01"
      endpoint:
        address: "10.10.0.13"

# Roteamento entre peers
routing:
  enabled: true
  export:
    export_all: false
    networks:
      - "172.16.10.0/24"   # Redes que este nÃ³ anuncia
      - "2001:db8:10::/64" # Suporte IPv6
    include_connected: true
    include_netplan_static: true
    metric: 100
  import:
    accept_all: false
    allow:
      - "172.16.0.0/16"
      - "2001:db8::/32"
    deny:
      - "0.0.0.0/0"        # Bloquear default route
    install:
      table: 100           # Tabela de roteamento customizada
      replace_existing: true
      flush_on_peer_down: true
      route_lease_seconds: 30

# Topologia
topology:
  mode: "direct-preferred"
  transit: "deny"          # NÃ£o permitir trÃ¢nsito por padrÃ£o

# SeguranÃ§a do control-plane
security:
  control_plane:
    transport: "grpc"
    listen:
      address: "0.0.0.0"
      port: 9898
    tls:
      enabled: false
      cert_file: "/etc/n-netman/tls/server.crt"
      key_file: "/etc/n-netman/tls/server.key"
      ca_file: "/etc/n-netman/tls/ca.crt"

# Observabilidade
observability:
  logging:
    level: "info"
    format: "json"
  metrics:
    enabled: true
    listen:
      address: "127.0.0.1"
      port: 9109
  healthcheck:
    enabled: true
    listen:
      address: "127.0.0.1"
      port: 9110
```

### Chaves PSK (Opcional)

Se usar autenticaÃ§Ã£o PSK entre peers:

```bash
sudo mkdir -p /etc/n-netman/psk

# Gerar chave para cada peer
openssl rand -hex 32 | sudo tee /etc/n-netman/psk/host-b-01.key
sudo chmod 600 /etc/n-netman/psk/*.key
```

### Certificados mTLS (Recomendado)

O `nnet` possui utilitÃ¡rios embutidos para gerenciar PKI:

```bash
# 1. Gerar CA Raiz
nnet cert init-ca --output-dir /etc/n-netman/tls

# 2. Gerar certificado para este nÃ³
nnet cert gen-host \
  --host $(hostname) \
  --ip 192.168.56.11 \
  --ca-cert /etc/n-netman/tls/ca.crt \
  --ca-key /etc/n-netman/tls/ca.key \
  --output-dir /etc/n-netman/tls
```

### Multi-Overlay (Config v2) ğŸ†•

A partir da versÃ£o 2 do config, vocÃª pode definir mÃºltiplos overlays VXLAN, cada um com seu prÃ³prio routing:

```yaml
version: 2

node:
  id: "host-a"
  hostname: "host-a"

overlays:
  # Production Overlay (VNI 100)
  - vni: 100
    name: "vxlan-prod"
    dstport: 4789
    mtu: 1450
    learning: true
    bridge:
      name: "br-prod"
      ipv4: "10.100.0.1/24"
    underlay_interface: "ens3"    # Interface fÃ­sica para este overlay
    bum:
      mode: "head-end-replication"
    routing:
      export:
        networks:
          - "172.16.10.0/24"
        metric: 100
      import:
        accept_all: true
        install:
          table: 100
          lookup_rules:
            enabled: true          # â† Cria rules de PBR automaticamente

  # Management Overlay (VNI 200)
  - vni: 200
    name: "vxlan-mgmt"
    dstport: 4789
    mtu: 1450
    learning: true
    bridge:
      name: "br-mgmt"
      ipv4: "10.200.1.1/24"
    underlay_interface: "ens4"
    bum:
      mode: "multicast"
      group: "239.1.1.200"
    routing:
      export:
        networks:
          - "10.200.0.0/24"
        metric: 200
      import:
        accept_all: true
        install:
          table: 200
          lookup_rules:
            enabled: true          # â† Cria rules de PBR automaticamente

# Peers (shared across overlays)
overlay:
  peers:
    - id: "host-b"
      endpoint:
        address: "192.168.56.12"
```

### Policy-Based Routing (`lookup_rules`) ğŸ†•

Quando `lookup_rules.enabled: true`, o n-netman cria automaticamente regras `ip rule` para direcionar trÃ¡fego da bridge para a tabela de roteamento correta:

```bash
# Regras criadas automaticamente para br-prod (table 100):
ip rule add iif br-prod lookup 100 priority 100
ip rule add oif br-prod lookup 100 priority 101

# Regras criadas automaticamente para br-mgmt (table 200):
ip rule add iif br-mgmt lookup 200 priority 100
ip rule add oif br-mgmt lookup 200 priority 101
```

**Por que isso Ã© importante?**

Sem essas regras, o kernel Linux consulta apenas a `table main` para decisÃµes de roteamento. Com `lookup_rules` habilitado:

1. TrÃ¡fego **entrando** por `br-prod` (iif) â†’ consulta `table 100`
2. TrÃ¡fego **saindo** por `br-prod` (oif) â†’ consulta `table 100`
3. Isolamento completo entre overlays â€” cada um usa sua prÃ³pria tabela

Veja o exemplo completo em [`examples/multi-overlay.yaml`](examples/multi-overlay.yaml).

---

## ğŸ® Uso

### CLI - Comandos DisponÃ­veis

```bash
# Ver ajuda
nnet --help

# Ver versÃ£o
nnet version

# Verificar configuraÃ§Ã£o e mostrar status
nnet -c /etc/n-netman/n-netman.yaml status

# Visualizar rotas configuradas
nnet -c /etc/n-netman/n-netman.yaml routes

# Dry-run (mostra o que seria feito sem executar)
nnet -c /etc/n-netman/n-netman.yaml apply --dry-run

# Aplicar configuraÃ§Ã£o (requer root)
sudo nnet -c /etc/n-netman/n-netman.yaml apply

# DiagnÃ³stico do sistema
nnet -c /etc/n-netman/n-netman.yaml doctor

# IntegraÃ§Ã£o libvirt - listar VMs
nnet libvirt list-vms --all

# IntegraÃ§Ã£o libvirt - attach VM a bridge
sudo nnet libvirt attach web-01 --bridge br-prod
```

### IntegraÃ§Ã£o libvirt

O n-netman oferece comandos para integrar VMs libvirt/KVM Ã s bridges de overlay:

![Fluxo de integraÃ§Ã£o](https://uml.nishisan.dev/proxy?src=https://raw.githubusercontent.com/nishisan-dev/n-netman/main/docs/diagrams/libvirt_integration.puml)

```bash
# Configurar dependÃªncia systemd (bridges existem antes das VMs)
sudo nnet libvirt enable

# Listar VMs e interfaces
nnet libvirt list-vms --all

# Attach VM a uma bridge
sudo nnet libvirt attach web-01 --bridge br-prod

# Detach por MAC
sudo nnet libvirt detach web-01 --mac 52:54:00:12:34:56

# Ver status da integraÃ§Ã£o
nnet libvirt status
```

Veja a documentaÃ§Ã£o completa em [docs/libvirt.md](docs/libvirt.md).

### Exemplo de SaÃ­da: `nnet status`

```
ğŸ–¥ï¸  Node: host-a (host-a)

ğŸ“¡ VXLAN Interfaces:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸŸ¢ UP vxlan100 (VNI 100, MTU 1450)

ğŸŒ‰ Bridges:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸŸ¢ UP br-nnet-100 (MTU 1450)
      Attached: [vxlan100]

ğŸ‘¥ Configured Peers:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ID      ENDPOINT       STATUS               ROUTES
  â”€â”€      â”€â”€â”€â”€â”€â”€â”€â”€       â”€â”€â”€â”€â”€â”€               â”€â”€â”€â”€â”€â”€
  host-b  192.168.56.12  ğŸŸ¢ healthy (5s ago)   1
  host-c  192.168.56.13  ğŸŸ¢ healthy (3s ago)   1

ğŸ“Š Route Statistics:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“¤ Exported:   1 route(s) (172.16.10.0/24)
  ğŸ“¥ Installed:  2 route(s) in table 100
      â€¢ 172.16.20.0/24 via 192.168.56.12 (host-b)
      â€¢ 172.16.30.0/24 via 192.168.56.13 (host-c)
```

### Daemon

```bash
# Iniciar daemon em foreground (requer root)
sudo nnetd -config /etc/n-netman/n-netman.yaml

# Ver versÃ£o
nnetd -version
```

### Systemd Service (Opcional)

Crie `/etc/systemd/system/n-netman.service`:

```ini
[Unit]
Description=n-netman VXLAN Overlay Manager
After=network-online.target
Wants=network-online.target

[Service]
Type=simple
ExecStart=/usr/local/bin/nnetd -config /etc/n-netman/n-netman.yaml
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
```

```bash
sudo systemctl daemon-reload
sudo systemctl enable n-netman
sudo systemctl start n-netman
sudo systemctl status n-netman
```

---

## ğŸ§ª Lab Testing (Vagrant)

O projeto inclui um `Vagrantfile` para testar multi-overlay em um ambiente com 3 VMs.

### Topologia do Lab (Multi-Overlay)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Underlay Networks                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    Production: 192.168.56.0/24    â”‚    Management: 192.168.57.0/24           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     host-a        â”‚       host-b        â”‚        host-c                       â”‚
â”‚  Prod: .56.11     â”‚    Prod: .56.12     â”‚     Prod: .56.13                    â”‚
â”‚  Mgmt: .57.11     â”‚    Mgmt: .57.12     â”‚     Mgmt: .57.13                    â”‚
â”‚                   â”‚                     â”‚                                     â”‚
â”‚ VNI 100 (Prod):   â”‚ VNI 100 (Prod):     â”‚ VNI 100 (Prod):                     â”‚
â”‚ 172.16.10.0/24    â”‚ 172.16.20.0/24      â”‚ 172.16.30.0/24                      â”‚
â”‚                   â”‚                     â”‚                                     â”‚
â”‚ VNI 200 (Mgmt):   â”‚ VNI 200 (Mgmt):     â”‚ VNI 200 (Mgmt):                     â”‚
â”‚ 10.200.10.0/24    â”‚ 10.200.20.0/24      â”‚ 10.200.30.0/24                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Requisitos

- [Vagrant](https://www.vagrantup.com/) instalado
- [VirtualBox](https://www.virtualbox.org/) instalado
- ~2GB de RAM livre

### Subir o Lab

```bash
# Subir as 3 VMs (primeira vez demora ~5min)
vagrant up

# Ver status
vagrant status
```

### Testar a Troca de Rotas

```bash
# Terminal 1: host-a
vagrant ssh host-a
sudo nnetd -config /etc/n-netman/n-netman.yaml

# Terminal 2: host-b
vagrant ssh host-b
sudo nnetd -config /etc/n-netman/n-netman.yaml

# Terminal 3: host-c
vagrant ssh host-c
sudo nnetd -config /etc/n-netman/n-netman.yaml
```

Aguarde ~30 segundos e verifique as rotas aprendidas:

```bash
# Em qualquer VM - verificar ambas as tabelas
ip route show table 100   # Production overlay routes
ip route show table 200   # Management overlay routes

# SaÃ­da esperada (ex: em host-a):
# Table 100 (Production - VNI 100):
# 172.16.20.0/24 via 10.100.0.2 dev br-prod proto openr metric 100
# 172.16.30.0/24 via 10.100.0.3 dev br-prod proto openr metric 100

# Table 200 (Management - VNI 200):
# 10.200.20.0/24 via 10.200.1.2 dev br-mgmt proto openr metric 200
# 10.200.30.0/24 via 10.200.1.3 dev br-mgmt proto openr metric 200
```

### Script de ValidaÃ§Ã£o

```bash
# Em cada VM, rodar o script de teste
./n-netman/scripts/lab-test.sh
```

### Comandos Ãšteis

```bash
# Destruir VMs
vagrant destroy -f

# Recriar uma VM especÃ­fica
vagrant destroy host-a -f && vagrant up host-a

# SSH em uma VM
vagrant ssh host-b
```

---

## ğŸ“Š Observabilidade

### MÃ©tricas Prometheus

DisponÃ­veis em `http://127.0.0.1:9109/metrics`. As seguintes mÃ©tricas sÃ£o coletadas e exportadas:

| MÃ©trica | DescriÃ§Ã£o |
|---------|-----------|
| `nnetman_reconciliations_total` | Total de ciclos de reconciliaÃ§Ã£o |
| `nnetman_reconciliation_errors_total` | Erros de reconciliaÃ§Ã£o |
| `nnetman_reconciliation_duration_seconds` | DuraÃ§Ã£o dos ciclos de reconciliaÃ§Ã£o |
| `nnetman_last_reconcile_timestamp_seconds` | Timestamp do Ãºltimo reconcile |
| `nnetman_vxlans_active` | Interfaces VXLAN ativas |
| `nnetman_bridges_active` | Bridges ativas |
| `nnetman_fdb_entries_total` | Total de entradas FDB |
| `nnetman_peers_configured` | Peers configurados |
| `nnetman_peers_connected` | Peers conectados |
| `nnetman_peers_healthy` | Peers saudÃ¡veis |
| `nnetman_routes_exported` | Rotas exportadas |
| `nnetman_routes_imported` | Rotas importadas |
| `nnetman_grpc_requests_total` | Total de requisiÃ§Ãµes gRPC |
| `nnetman_grpc_request_duration_seconds` | DuraÃ§Ã£o das requisiÃ§Ãµes gRPC |

### Health Checks

```bash
# Liveness
curl http://127.0.0.1:9110/livez

# Readiness
curl http://127.0.0.1:9110/readyz

# Health geral
curl http://127.0.0.1:9110/healthz

# Status detalhado (peers + rotas)
curl http://127.0.0.1:9110/status
```

---

## ğŸ§© Componentes Internos (Go)

- `cmd/nnetd`: daemon (carrega config, inicia observabilidade e reconciler)
- `cmd/nnet`: CLI para aplicar config e inspecionar estado
- `internal/config`: structs, defaults e validaÃ§Ã£o do YAML
- `internal/reconciler`: loop que garante bridge/VXLAN/FDB conforme config
- `internal/netlink`: wrappers de bridge/VXLAN/FDB/rotas via netlink
- `internal/controlplane`: servidor/cliente gRPC com ExchangeState/Announce/Withdraw
- `internal/routing`: polÃ­ticas de export/import (helpers, ainda nÃ£o aplicados no daemon)
- `internal/observability`: mÃ©tricas Prometheus e healthchecks HTTP

---

## ğŸ”§ Troubleshooting

### Verificar Interfaces Criadas

```bash
# VXLAN
ip -d link show vxlan100

# Bridge
ip -d link show br-nnet-100
bridge link show

# FDB entries
bridge fdb show dev vxlan100
```

### Verificar Rotas

```bash
# Rotas na tabela 100
ip route show table 100

# Todas as rotas
ip route show
```

### Logs

```bash
# Com systemd
journalctl -u n-netman -f

# Em foreground
nnetd -config /etc/n-netman/n-netman.yaml 2>&1 | jq .
```

### DiagnÃ³stico Completo

```bash
nnet doctor
```

---

## ğŸ—ï¸ Arquitetura

### VisÃ£o Geral dos Componentes

O diagrama abaixo mostra a arquitetura atual. O control-plane agora implementa troca real de rotas via gRPC.

Fonte: `docs/diagrams/architecture.puml`

![Arquitetura geral](https://uml.nishisan.dev/proxy?src=https://raw.githubusercontent.com/nishisan-dev/n-netman/main/docs/diagrams/architecture.puml)

### Fluxo de ReconciliaÃ§Ã£o (Multi-Overlay)

Fonte: `docs/diagrams/reconciler-loop.puml`

![Fluxo de reconciliacao](https://uml.nishisan.dev/proxy?src=https://raw.githubusercontent.com/nishisan-dev/n-netman/main/docs/diagrams/reconciler-loop.puml)

### Troca de Rotas entre Peers

Fonte: `docs/diagrams/route-exchange.puml`

![Troca de rotas entre peers](https://uml.nishisan.dev/proxy?src=https://raw.githubusercontent.com/nishisan-dev/n-netman/main/docs/diagrams/route-exchange.puml)

### Topologia de Rede

Fonte: `docs/diagrams/topology.puml`

![Topologia de rede](https://uml.nishisan.dev/proxy?src=https://raw.githubusercontent.com/nishisan-dev/n-netman/main/docs/diagrams/topology.puml)

---

## âš ï¸ LimitaÃ§Ãµes Atuais (MVP)

Esta Ã© uma versÃ£o MVP. As seguintes funcionalidades **ainda nÃ£o estÃ£o implementadas**:

### NÃ£o Funcional
| Item | Status | DescriÃ§Ã£o |
|------|--------|-----------|
| **ValidaÃ§Ã£o de PSK** | âŒ | Chaves PSK sÃ£o lidas mas nÃ£o validadas |
| **Netplan parsing** | âŒ | Rotas do netplan nÃ£o sÃ£o lidas automaticamente |
| **PolÃ­ticas de import/export** | âŒ | `allow/deny`, `include_connected`, `include_netplan_static` ainda nÃ£o sÃ£o aplicados |

### Funcional
| Item | Status | DescriÃ§Ã£o |
|------|--------|-----------|
| **VXLAN/Bridge** | âœ… | CriaÃ§Ã£o funciona (requer root) |
| **FDB entries** | âœ… | SincronizaÃ§Ã£o de peers funciona |
| **Troca de rotas gRPC** | âœ… | Handlers implementados, rotas instaladas |
| **TLS/mTLS** | âœ… | ComunicaÃ§Ã£o gRPC criptografada entre peers |
| **Multi-Overlay** | âœ… | VNI-aware routing com tabelas independentes por overlay |
| **Reconciler** | âœ… | Loop funciona |
| **MÃ©tricas** | âœ… | Servidor Prometheus ativo e mÃ©tricas coletadas |
| **Healthcheck** | âœ… | Endpoints funcionam |
| **Status de peers** | âœ… | Health check implementado com keepalive |
| **IntegraÃ§Ã£o libvirt** | âœ… | CLI `nnet libvirt` para attach/detach de VMs |

### PrÃ³ximas Prioridades
1. Testes de integraÃ§Ã£o com VMs reais em lab
2. ValidaÃ§Ã£o de PSK entre peers
3. Policies avanÃ§adas de import/export

---

## ğŸ› Known Issues

### FDB Entries via Netlink Library

A biblioteca Go `vishvananda/netlink` possui um bug conhecido onde `NeighAppend()` retorna "operation not supported" ao adicionar entradas FDB em interfaces VXLAN que estÃ£o attached a uma bridge.

**Workaround implementado:** Utilizamos `exec.Command("bridge", "fdb", "append", ...)` diretamente ao invÃ©s da API da biblioteca.

**ReferÃªncia:** [vishvananda/netlink#714](https://github.com/vishvananda/netlink/issues/714)

---

## ğŸ“œ LicenÃ§a

Nishi Network Manager License (Non-Commercial Evaluation) - veja [LICENSE](LICENSE) para detalhes.

> **Nota:** Uso comercial requer licenÃ§a separada. Contate o Licensor para mais informaÃ§Ãµes.

---

## ğŸ¤ Contribuindo

1. Fork o repositÃ³rio
2. Crie uma branch (`git checkout -b feature/minha-feature`)
3. Commit suas mudanÃ§as (`git commit -am 'feat: adiciona minha feature'`)
4. Push para a branch (`git push origin feature/minha-feature`)
5. Abra um Pull Request
